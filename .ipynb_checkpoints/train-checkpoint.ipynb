{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Utility used by the Network class to actually train.\n",
    "\n",
    "Based on:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Metric\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek, futureWeek=1, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        \n",
    "        X = np.array(train.iloc[i:i+pastWeek,:])\n",
    "        \n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return (np.array(X_train), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(timeLag):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, timeLag)\n",
    "    \n",
    "    ## Data split\n",
    "    x_train = x_data[0:int(x_data.shape[0]*0.8)]\n",
    "    x_test = x_data[int(x_data.shape[0]*0.8):]\n",
    "    \n",
    "    y_train = y_data[0:int(y_data.shape[0]*0.8)]\n",
    "    y_test = y_data[int(y_data.shape[0]*0.8):]\n",
    "    \n",
    "    ## Other information\n",
    "    nb_output = 1\n",
    "    \n",
    "    return (nb_output, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"WeeklyFinalData.csv\"\n",
    "# data = read(path)\n",
    "\n",
    "# date = data[\"Date\"]\n",
    "# data.drop(\"Date\", axis=1, inplace=True)\n",
    "\n",
    "# ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "# x_data, y_data = buildTrain(data, 30)\n",
    "# print(x_data.shape)\n",
    "# # print(x_data.reshape(-1,3,15)[0])\n",
    "# # print(y_data[0])\n",
    "# # print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(nb_neurons, nb_layers, optimizer, nb_output, input_shape):\n",
    "    \"\"\"Compile a sequential model.\n",
    "\n",
    "    Args:\n",
    "        network (dict): the parameters of the network\n",
    "\n",
    "    Returns:\n",
    "        a compiled network.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get our network parameters.\n",
    "    keras.backend.clear_session()\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add each layer.\n",
    "    for i in range(nb_layers):\n",
    "\n",
    "        # Need input shape for first layer.\n",
    "        if i == 0:\n",
    "            model.add(GRU(units = nb_neurons, batch_input_shape=input_shape, return_sequences=True))\n",
    "        if i==(nb_layers-1):\n",
    "            model.add(GRU(units = nb_neurons, batch_input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(GRU(units = nb_neurons, return_sequences=True))\n",
    "\n",
    "#         model.add(Dropout(0.2))  # hard-coded dropout\n",
    "\n",
    "    # Output layer.\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(units = nb_output))\n",
    "\n",
    "#     print(model.summary())\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(Network):\n",
    "    \"\"\"Train the model, return test loss.\n",
    "\n",
    "    Args:\n",
    "        network (dict): the parameters of the network\n",
    "        dataset (str): Dataset to use for training/evaluating\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ## The moving window mechanism for incremental learning (# of batch)\n",
    "    batch_size = Network.network['batch_size']\n",
    "    \n",
    "    ## The time lag as forecasting variable (# of sequence)\n",
    "    window_size = Network.network['window_size']\n",
    "    \n",
    "    ## The hyperparameter for GRU\n",
    "    nb_neurons = Network.network['nb_neurons']\n",
    "    nb_layers = Network.network['nb_layers']\n",
    "    optimizer = Network.network['optimizer']\n",
    "\n",
    "    ## Get the training data from help method: get_data(window_size)\n",
    "    nb_output, x_train, x_test, y_train, y_test = get_data(window_size)\n",
    "    \n",
    "    ## The number of forecasting variable (# of variable)\n",
    "    nb_input_factor = 15\n",
    "    \n",
    "    ## Data transformation\n",
    "    x_train_scaled = sc.fit_transform(x_train).reshape(-1,window_size,nb_input_factor)\n",
    "    x_test_scaled = sc.transform(x_test).reshape(-1,window_size,nb_input_factor)\n",
    "    y_train_scaled = sc.fit_transform(y_train)\n",
    "    \n",
    "    ## Define the GRU input_shape and compile the model\n",
    "    input_shape = (None, window_size, nb_input_factor)\n",
    "    model = compile_model(nb_neurons, nb_layers, optimizer, nb_output, input_shape)\n",
    "    \n",
    "    ## The volume of training data\n",
    "    nb_data = x_train_scaled.shape[0]\n",
    "    \n",
    "    ## The performance_indicator list to store loss and accuracy\n",
    "    performance_indicator = pd.DataFrame(columns=[\"Loss_train\",\"Accuracy_train\",\"Loss_test\",\"Accuracy_test\"])\n",
    "    \n",
    "    ## The times of training\n",
    "    epoch = 300\n",
    "    \n",
    "    ## The training step of GRU\n",
    "    for e in range(epoch):\n",
    "        \n",
    "        minimum_loss = np.inf\n",
    "        current_times = 0\n",
    "\n",
    "        if (current_times > 5):\n",
    "\n",
    "            model.load_weights(\"model_weight.h5\")\n",
    "            Network.set_model(model)\n",
    "           # Network.set_weights(model.get_weights())\n",
    "            break\n",
    "\n",
    "        else:\n",
    "\n",
    "            for i in range(0, nb_data-batch_size+1):\n",
    "\n",
    "                end = i + batch_size\n",
    "\n",
    "                if end < nb_data:\n",
    "                    x = x_train_scaled[i:end]\n",
    "                    y = y_train_scaled[i:end]\n",
    "#                     print(x.shape,y.shape)\n",
    "                    model.train_on_batch(x, y)\n",
    "\n",
    "                else:\n",
    "                    x = x_train_scaled[i:nb_data]\n",
    "                    y = y_train_scaled[i:nb_data]\n",
    "#                     print(i)\n",
    "#                     print(x.shape, y.shape)\n",
    "                    model.train_on_batch(x, y)\n",
    "\n",
    "            ## To calculate the forecasting performance\n",
    "            y_pred_train = sc.inverse_transform(model.predict(x_train_scaled))\n",
    "            loss_train = tf.reduce_mean(tf.square(y_train - y_pred_train)).numpy()\n",
    "            accuracy_train = tf.reduce_sum(tf.cast(tf.less_equal(tf.abs(y_train - y_pred_train), 500), dtype = tf.float32)).numpy()/y_train.shape[0]\n",
    "\n",
    "            y_pred_test = sc.inverse_transform(model.predict(x_test_scaled))\n",
    "            loss_test = tf.reduce_mean(tf.square(y_test - y_pred_test)).numpy()\n",
    "            accuracy_test = tf.reduce_sum(tf.cast(tf.less_equal(tf.abs(y_test - y_pred_test), 500), dtype = tf.float32)).numpy()/y_test.shape[0]\n",
    "\n",
    "            epoch_performance = pd.DataFrame({\n",
    "                \"Loss_train\":loss_train,\n",
    "                \"Accuracy_train\":accuracy_train,\n",
    "                \"Loss_test\":loss_test,\n",
    "                \"Accuracy_test\":accuracy_test\n",
    "            },index=[0])\n",
    "\n",
    "            performance_indicator = performance_indicator.append(epoch_performance,ignore_index=True)\n",
    "            print(\"Epoch: %d, Loss: %.2f, Accuracy: %.2f%%\"%(e,loss_train,accuracy_train*100))\n",
    "            print(\"-\"*50)\n",
    "\n",
    "\n",
    "            if(loss_train <= minimum_loss):\n",
    "                minimum_loss = loss_train\n",
    "                current_times=0\n",
    "                model.save_weights(\"model_weight.h5\")\n",
    "                Network.set_model(model)\n",
    "\n",
    "            else:\n",
    "                current_times += 1\n",
    "                if (e>=epoch-1):\n",
    "                    Network.set_model(model)\n",
    "    \n",
    "    Network.performance_indicator = performance_indicator\n",
    "    \n",
    "    \n",
    "    y_pred_test = sc.inverse_transform(model.predict(x_test_scaled))\n",
    "    loss_test = tf.reduce_mean(tf.square(y_test - y_pred_test)).numpy()\n",
    "    accuracy_test = tf.reduce_sum(tf.cast(tf.less_equal(tf.abs(y_test - y_pred_test), 500), dtype = tf.float32)).numpy()/y_tset.shape[0]\n",
    "    \n",
    "    Network.y_true = y_test\n",
    "    Network.y_predict = y_pred_test\n",
    "    \n",
    "    return accuracy_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Class that represents the network to be evolved.\"\"\"\n",
    "# import random\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Network():\n",
    "#     \"\"\"Represent a network and let us operate on it.\n",
    "\n",
    "#     Currently only works for an MLP.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, nn_param_choices=None):\n",
    "#         \"\"\"Initialize our network.\n",
    "\n",
    "#         Args:\n",
    "#             nn_param_choices (dict): Parameters for the network, includes:\n",
    "#                 'window_size':[i for i in range(1,50)]\n",
    "#                 'nb_neurons': [i for i in range(3, 41, 1)],\n",
    "#                 'nb_layers': [i for i in range(1,11)],\n",
    "#                 'batch_size':[i for i in range(1,21)],\n",
    "#                 'epoch':[i for i in range(10,501)],\n",
    "#                 'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n",
    "#                                   'adadelta', 'adamax', 'nadam','ftrl'],\n",
    "#         \"\"\"\n",
    "#         self.accuracy = 0.\n",
    "#         self.nn_param_choices = nn_param_choices\n",
    "#         self.network = {}  # (dic): represents MLP network parameters\n",
    "#         self.model = None\n",
    "#         self.performance_indicator = None\n",
    "#         self.y_true = None\n",
    "#         self.y_predict = None\n",
    "        \n",
    "#     def create_random(self):\n",
    "#         \"\"\"Create a random network.\"\"\"\n",
    "#         for key in self.nn_param_choices:\n",
    "#             self.network[key] = random.choice(self.nn_param_choices[key])\n",
    "\n",
    "#     def create_set(self, network):\n",
    "#         \"\"\"Set network properties.\n",
    "\n",
    "#         Args:\n",
    "#             network (dict): The network parameters\n",
    "\n",
    "#         \"\"\"\n",
    "#         self.network = network\n",
    "\n",
    "#     def train(self):\n",
    "#         \"\"\"Train the network and record the accuracy.\n",
    "\n",
    "#         Args:\n",
    "#             dataset (str): Name of dataset to use.\n",
    "\n",
    "#         \"\"\"\n",
    "#         if self.accuracy == 0.:\n",
    "#             self.accuracy = train_and_score(self)\n",
    "\n",
    "#     def set_model(self, trained_model):\n",
    "#         self.model = trained_model\n",
    "    \n",
    "#     def plot_learning_graph (self):\n",
    "    \n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n",
    "\n",
    "#         data = self.performance_indicator\n",
    "#         for i in range(0,2):\n",
    "#             ax = axes[i]\n",
    "\n",
    "#             train = data.iloc[:,i]\n",
    "#             test = data.iloc[:,i+2]\n",
    "\n",
    "#             ax.plot(train, label=\"Train\")\n",
    "#             ax.plot(test, label=\"Test\")\n",
    "#             ax.legend()\n",
    "#             ax.set_xlabel(\"epoch\")\n",
    "\n",
    "#             if i==0:\n",
    "#                 ax.set_title(\"Loss\")\n",
    "#                 ax.set_ylabel(\"Loss\")\n",
    "#             else:\n",
    "#                 ax.set_title(\"Accuracy\")\n",
    "#                 ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "#             ax.ticklabel_format(style='plain', useOffset=False, axis='both')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "        \n",
    "        \n",
    "#     def plot_prediction_value(self):\n",
    "    \n",
    "#         fig, axes = plt.subplots(figsize=(15, 5), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n",
    "\n",
    "#         axes.plot(self.y_true, label=\"Actual\")\n",
    "#         axes.plot(self.y_predict, label=\"Prediction\")\n",
    "#         axes.legend()\n",
    "#         axes.set_xlabel(\"Sample index(Weekly)\")\n",
    "#         axes.set_ylabel(\"Changjiang Copper Spot Price $CNY/ton\")\n",
    "#         axes.set_title(\"Actual data and predicted data comparison\")\n",
    "#         axes.ticklabel_format(style='plain', useOffset=False, axis='both')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "    \n",
    "#     def print_network(self):\n",
    "#         \"\"\"Print out a network.\"\"\"\n",
    "#         logging.info(self.network)\n",
    "#         logging.info(\"Network accuracy: %.2f%%\" % (self.accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_param_choices = {\n",
    "    \n",
    "#     'window_size':[20],\n",
    "#     'batch_size':[i for i in range(4,9)],\n",
    "#     'nb_neurons': [i for i in range(3, 41, 1)],\n",
    "#     'nb_layers': [i for i in range(1,11)],\n",
    "#     'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n",
    "#                   'adadelta', 'adamax', 'nadam','ftrl']\n",
    "# }\n",
    "\n",
    "# network = Network(nn_param_choices)\n",
    "# network.create_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.plot_learning_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.plot_prediction_value()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
